{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991a329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2cb242f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profilesID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f767d5cc7681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofilesID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'profilesID' is not defined"
     ]
    }
   ],
   "source": [
    "len(profilesID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab10556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install simplelist\n",
    "from simplelist import listfromtxt\n",
    "from simplelist import txtfromlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef37688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a00931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf3218f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profilesID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ff3793db8dd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtxtfromlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ProfilesID13.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofilesID\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Read A Python List Into A TXT File\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'profilesID' is not defined"
     ]
    }
   ],
   "source": [
    "txtfromlist('ProfilesID13.txt', profilesID) # Read A Python List Into A TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba43b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d7e273",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Lists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-45101d9cf6e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mProfileIDS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mprofileID\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mLists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m   \u001b[0mfulllink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://www.linkedin.com'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprofileID\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfulllink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Lists' is not defined"
     ]
    }
   ],
   "source": [
    "# For loop to iterate over each URL in the list\n",
    "from parsel import Selector\n",
    "\n",
    "\n",
    "names=[]\n",
    "Current_Company =[]\n",
    "Current_Job=[]\n",
    "Total_year=[]\n",
    "Total_Experience=[]\n",
    "job_location=[]\n",
    "Last_University=[]\n",
    "Last_degree=[]\n",
    "Graduation_start_year=[]\n",
    "Graduation_end_year=[]\n",
    "skills_set=[]\n",
    "ProfileIDS=[]\n",
    "\n",
    "for profileID in Lists:\n",
    "  fulllink = 'https://www.linkedin.com' + profileID\n",
    "  driver.get(fulllink)\n",
    "  time.sleep(5)\n",
    "  src = driver.page_source\n",
    "  soup = BeautifulSoup(src, 'lxml')\n",
    "    # open profile URL\n",
    "    #driver.get(profileID)\n",
    "\n",
    "    # add a 3 second pause loading each URL\n",
    "    #time.sleep(8)\n",
    "    \n",
    "  total_height = int(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "\n",
    "  for i in range(1, total_height, 5):\n",
    "        driver.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "\n",
    "    # assigning the source code for the webpage to variable sel\n",
    "  sel = Selector(text=driver.page_source)\n",
    "\n",
    "  name = sel.xpath('//*[starts-with(@class, \"text-heading-xlarge inline t-24 v-align-middle break-words\")]/text()').extract_first()\n",
    "  if name:\n",
    "      name = name.strip()\n",
    "  names.append(name)\n",
    "      #print(name)\n",
    "    \n",
    "  #scraping companies\n",
    "  #company = soup.find('div', {'aria-label':\"Current company\" }).extract_first()\n",
    "  company = sel.xpath('//*[starts-with(@class, \"pv-entity__secondary-title t-14 t-black t-normal\")]/text()').extract_first()\n",
    "  if company:\n",
    "    company = company.strip()\n",
    "  Current_Company.append(company)\n",
    "\n",
    "\n",
    "  #job_title = sel.xpath('//*[starts-with(@class, \"t-16 t-black t-bold\")]/text()').extract_first()\n",
    "  job_title = sel.xpath('//*[starts-with(@class, \"t-16 t-black t-bold\")]/text()').extract_first()\n",
    "  if job_title:\n",
    "      job_title = job_title.strip()\n",
    "  Current_Job.append(job_title)\n",
    "      #print(jobtitle)\n",
    "    \n",
    "  # job location\n",
    "  job_locations = sel.xpath('//*[starts-with(@class, \"pv-entity__location t-14 t-black--light t-normal block\")]/span[2]/text()').extract_first()\n",
    "  #job_locations = job.find(\"span\", class_=\"job-result-card__location\").text\n",
    "  if job_locations:\n",
    "      job_locations = job_locations.strip()\n",
    "  job_location.append(job_locations)\n",
    "  # print(Job location)\n",
    "  #else: \n",
    "     #job_locations = sel.xpath('//*[starts-with(@class, \"text-body-small inline t-black--light break-words\")]/text()').extract_first()\n",
    "         #if job_locations:\n",
    "             #location = job_locations.strip()\n",
    "         #job_location.append(job_locations)\n",
    "      # print(location)\n",
    "        \n",
    "        \n",
    "  University = sel.xpath('//*[starts-with(@class, \"pv-entity__school-name t-16 t-black t-bold\")]/text()').extract_first()\n",
    "  if University:\n",
    "      University = University.strip()\n",
    "  Last_University.append(University)\n",
    "      # print(location)\n",
    "    \n",
    "    \n",
    "          \n",
    "  degree = sel.xpath('//*[starts-with(@class, \"pv-entity__comma-item\")]/text()').extract_first()\n",
    "  if degree:\n",
    "      degree = degree.strip()\n",
    "  Last_degree.append(degree)\n",
    "      # print(last Degree)\n",
    "    \n",
    "  Grad_start_year = sel.xpath('//*[starts-with(@class, \"pv-entity__dates t-14 t-black--light t-normal\")]/span[2]/time[1]/text()').extract_first()\n",
    "  if Grad_start_year:\n",
    "     Grad_start_year = Grad_start_year.strip()\n",
    "  Graduation_start_year.append(Grad_start_year)\n",
    "      # print(Grad_end_year)\n",
    "    \n",
    "    \n",
    "  Grad_end_year = sel.xpath('//*[starts-with(@class, \"pv-entity__dates t-14 t-black--light t-normal\")]/span[2]/time[2]/text()').extract_first()\n",
    "  if Grad_end_year:\n",
    "     Grad_end_year = Grad_end_year.strip()\n",
    "  Graduation_end_year.append(Grad_end_year)\n",
    "      # print(Grad_end_year)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  skills = sel.xpath('//*[starts-with(@class, \"pv-skill-category-entity__name-text t-16 t-black t-bold\")]/text()').extract_first()\n",
    "  if skills:\n",
    "     skills = skills.strip()\n",
    "  skills_set.append(skills)\n",
    "      # print(skills)\n",
    "    \n",
    "    \n",
    "#class=\"pv-skill-category-entity__name-text t-16 t-black t-bold\"\n",
    "#pv-skill-category-entity__name tooltip-container\n",
    "    \n",
    "    \n",
    "\n",
    "  Dates_Employed = sel.xpath('//*[starts-with(@class, \"pv-entity__date-range t-14 t-black--light t-normal\")]/span[2]/text()').extract_first()\n",
    "  if Dates_Employed:\n",
    "     Dates_Employed = Dates_Employed.strip()\n",
    "  Total_year.append(Dates_Employed)\n",
    "      # print(Dates_Employed)\n",
    "    \n",
    "    \n",
    "  Experience = sel.xpath('//*[starts-with(@class, \"pv-entity__bullet-item-v2\")]/text()').extract_first()\n",
    "  if Experience:\n",
    "        Experience = Experience.strip()\n",
    "  Total_Experience.append(Experience)\n",
    "      # print(Experience)\n",
    "     \n",
    " \n",
    " #sel.getAttribue(\"//img/\"src\")\n",
    "  #image = sel.xpath('//img[contains(@class, \"presence-entity__image presence- entity__image\")]/img[]/src()').extract_first()\n",
    "  #if image:\n",
    "  #   image = image.strip()\n",
    " # profile_image.append(image)\n",
    "      # print(image)    \n",
    "\n",
    "  profileID = driver.current_url\n",
    "  ProfileIDS.append(profileID)\n",
    "    \n",
    "\n",
    "\n",
    "  # Print what information is being write to the CSV file\n",
    "  rows = [name,company,job_title,Total_year,Total_Experience,job_location,degree,Graduation_start_year,Graduation_end_year,skills_set,University,profileID]\n",
    " # print('Writing to CSV:', rows)\n",
    "  df = pd.DataFrame(list(zip(names,Current_Company,Current_Job,Total_year,Total_Experience,job_location,Last_degree,Graduation_start_year,Graduation_end_year,skills_set,Last_University,ProfileIDS)), \n",
    "               columns =['Name', 'Current_Company', 'Current_Job', 'Total_year',   'Total_Experience',  'job_location',  'Last_degree' ,  'Graduation_start_year', 'Graduation_end_year','skills_set',  'Last_University', 'ProfileIDS'])\n",
    "    \n",
    "  #print(df)\n",
    "  df.to_csv('./Scrapdata1111.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bd15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b2f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7d68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661ccb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02037714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5c0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ae2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d48251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d343e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e46137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b85ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ce75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f9797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c048f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816e29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a074736c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-91166f547b74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[0mname_div\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'flex-1 mr5'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m \u001b[0mname_loc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ul'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[0mfullname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname_loc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Initialize Options to start Chrome as headless in selenium\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "capabilities = DesiredCapabilities.CHROME.copy()\n",
    "capabilities['acceptSslCerts'] = True\n",
    "capabilities['acceptInsecureCerts'] = True\n",
    "\n",
    "# Initialize the chrome webdriver as 'browser'\n",
    "browser = webdriver.Chrome(options=chrome_options, desired_capabilities=capabilities)\n",
    "\n",
    "# To initialize the webdriver in a new window for Debugging:\n",
    "# browser = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "# Get the login page for linkedin\n",
    "browser.get('https://www.linkedin.com/uas/login')\n",
    "\n",
    "# Open the file with the username and password for LinkedIn login\n",
    "file = open('config.txt')\n",
    "lines = file.readlines()\n",
    "username = lines[0]\n",
    "password = lines[1]\n",
    "\n",
    "# Username and Password for login\n",
    "elementID = browser.find_element_by_id('username')\n",
    "elementID.send_keys(username)\n",
    "\n",
    "elementID = browser.find_element_by_id('password')\n",
    "elementID.send_keys(password)\n",
    "\n",
    "elementID.submit()\n",
    "time.sleep(5)\n",
    "\n",
    "# Profile Link to be scraped\n",
    "link = \"https://www.linkedin.com/in/premnagdeo/\"\n",
    "browser.get(link)\n",
    "\n",
    "# pause before scrolling\n",
    "SCROLL_PAUSE_TIME = 6\n",
    "\n",
    "# Get the scroll height of the page\n",
    "last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# scroll the entire page due to dynamic loading of the webpage we need to load the entire webpage by scrolling\n",
    "for i in range(3):\n",
    "    # Scroll down to bottom\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight/3);\")\n",
    "    time.sleep(SCROLL_PAUSE_TIME / 2)\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight*(2/3));\")\n",
    "    time.sleep(SCROLL_PAUSE_TIME / 2)\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    # Wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# try to expand sections(if available), else pass\n",
    "try:\n",
    "    # click to expand education section\n",
    "    education_expand_button = browser.find_element_by_xpath(\n",
    "        \"//section[@id='education-section']//button[@class='pv-profile-section__see-more-inline pv-profile-section__text-truncate-toggle link link-without-hover-state']\")\n",
    "    browser.execute_script(\"arguments[0].click();\", education_expand_button)\n",
    "except Exception as e:\n",
    "    # print(\"education_expand_button Exception:\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # click to expand projects section\n",
    "    projects_expand_button = browser.find_element_by_xpath(\n",
    "        \"//div[@class='pv-accomplishments-block__content break-words']//button[@aria-label='Expand projects section' and @aria-expanded='false']\")\n",
    "    browser.execute_script(\"arguments[0].click();\", projects_expand_button)\n",
    "except Exception as e:\n",
    "    # print(\"projects_expand_button Exception:\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # click to expand certifications section\n",
    "    certifications_expand_button = browser.find_element_by_xpath(\n",
    "        \"//button[@class='pv-profile-section__see-more-inline pv-profile-section__text-truncate-toggle link link-without-hover-state']\")\n",
    "    browser.execute_script(\"arguments[0].click();\", certifications_expand_button)\n",
    "except Exception as e:\n",
    "    # print(\"certifications_expand_button Exception:\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # click to expand experience section\n",
    "    experiences_expand_button = browser.find_element_by_xpath(\n",
    "        \"//button[@class='pv-profile-section__see-more-inline pv-profile-section__text-truncate-toggle link link-without-hover-state']\")\n",
    "    browser.execute_script(\"arguments[0].click();\", experiences_expand_button)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    # inline-show-more-text__button link\n",
    "    experiences_show_more_expand_button = browser.find_element_by_xpath(\"//button[@class='inline-show-more-text__button link']\")\n",
    "    # print(experiences_show_more_expand_button)\n",
    "    browser.execute_script(\"arguments[0].click();\", experiences_show_more_expand_button)\n",
    "except Exception as e:\n",
    "    # print(\"experiences_expand_button Exception:\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # click to expand skills section\n",
    "    skills_expand_button = browser.find_element_by_xpath(\n",
    "        \"//button[@class='pv-profile-section__card-action-bar pv-skills-section__additional-skills artdeco-container-card-action-bar artdeco-button artdeco-button--tertiary artdeco-button--3 artdeco-button--fluid']\")\n",
    "    browser.execute_script(\"arguments[0].click();\", skills_expand_button)\n",
    "except Exception as e:\n",
    "    # print(\"skills_expand_button Exception:\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # click to expand volunteering section\n",
    "    volunteer_expand_button = browser.find_element_by_xpath(\n",
    "        \"//button[@class='pv-profile-section__see-more-inline pv-profile-section__text-truncate-toggle link link-without-hover-state']\")\n",
    "    browser.execute_script(\"arguments[0].click();\", volunteer_expand_button)\n",
    "except Exception as e:\n",
    "    # print(\"volunteer_expand_button Exception:\", e)\n",
    "    pass\n",
    "\n",
    "# use beautiful soup for html parsing\n",
    "src = browser.page_source\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "# BASIC INFO LIST\n",
    "basic_info_list = []\n",
    "\n",
    "name_div = soup.find('div', {'class': 'flex-1 mr5'})\n",
    "name_loc = name_div.find_all('ul')\n",
    "fullname = name_loc[0].find('li').get_text().strip()\n",
    "try:\n",
    "    first_name, last_name = fullname.split()\n",
    "# above statement fails when a person has put their name as firstname, middlename, lastname\n",
    "except:\n",
    "    first_name, middle_name, last_name = fullname.split()\n",
    "\n",
    "basic_info_list.append(first_name)\n",
    "basic_info_list.append(last_name)\n",
    "\n",
    "headline = name_div.find('h2').get_text().strip()\n",
    "basic_info_list.append(headline)\n",
    "basic_info_list.append(link)\n",
    "\n",
    "# print(basic_info_list)\n",
    "\n",
    "# education section\n",
    "education_info_list = []\n",
    "try:\n",
    "    edu_section = soup.find('section', {'id': 'education-section'}).find('ul')\n",
    "    edu_section = edu_section.find_all('div', {'class': 'pv-entity__summary-info pv-entity__summary-info--background-section'})\n",
    "    college_names = []\n",
    "    degree_names = []\n",
    "    field_names = []\n",
    "    grades = []\n",
    "    dates = []\n",
    "    for x in range(len(edu_section)):\n",
    "        curr_section = edu_section[x]\n",
    "        try:\n",
    "            college_name = curr_section.find('h3', {'class': 'pv-entity__school-name t-16 t-black t-bold'})\n",
    "            college_names.append(college_name.get_text())\n",
    "        except Exception as e:\n",
    "            # print(\"Education college_name Exception\",e)\n",
    "            college_names.append('')\n",
    "\n",
    "        try:\n",
    "            degree_name = curr_section.find('p', {'class': 'pv-entity__secondary-title pv-entity__degree-name t-14 t-black t-normal'}).find('span', {\n",
    "                'class': 'pv-entity__comma-item'})\n",
    "            degree_names.append(degree_name.get_text())\n",
    "        except Exception as e:\n",
    "            # print(\"Education degree_name Exception\",e)\n",
    "            degree_names.append('')\n",
    "\n",
    "        try:\n",
    "            field_name = curr_section.find('p', {'class': 'pv-entity__secondary-title pv-entity__fos t-14 t-black t-normal'}).find('span', {\n",
    "                'class': 'pv-entity__comma-item'})\n",
    "            field_names.append(field_name.get_text())\n",
    "        except Exception as e:\n",
    "            # print(\"Education field_name Exception\",e)\n",
    "            field_names.append('')\n",
    "\n",
    "        try:\n",
    "            grade = curr_section.find('p', {'class': 'pv-entity__secondary-title pv-entity__grade t-14 t-black t-normal'}).find('span', {\n",
    "                'class': 'pv-entity__comma-item'})\n",
    "            grades.append(grade.get_text())\n",
    "        except Exception as e:\n",
    "            # print(\"Education grade Exception\",e)\n",
    "            grades.append('')\n",
    "\n",
    "        try:\n",
    "            time = curr_section.find('p', {'class': 'pv-entity__dates t-14 t-black--light t-normal'})\n",
    "            dates.append((time.find_all('time')[1].get_text()))\n",
    "        except Exception as e:\n",
    "            # print(\"Education time Exception\",e)\n",
    "            dates.append('')\n",
    "\n",
    "    for i in range(len(edu_section)):\n",
    "        education_info_list.append([college_names[i], degree_names[i], field_names[i], dates[i], grades[i]])\n",
    "except Exception as e:\n",
    "    # no education added\n",
    "    # print(\"Education Section Exception\", e)\n",
    "    pass\n",
    "\n",
    "# print(education_info_list)\n",
    "\n",
    "\n",
    "# Project Section\n",
    "projects_info_list = []\n",
    "project_titles = []\n",
    "try:\n",
    "    project_section = soup.find('div', {'id': 'projects-expandable-content'})\n",
    "    project_section = project_section.find('ul', {'class': 'pv-accomplishments-block__list'})\n",
    "\n",
    "    projects = project_section.find_all('h4', {'class': 'pv-accomplishment-entity__title t-14 t-bold'})\n",
    "\n",
    "    for i in range(len(projects)):\n",
    "        project_name = projects[i].get_text().split('\\n')[2]\n",
    "        project_name = re.sub(' +', ' ', project_name)\n",
    "        project_titles.append(project_name.strip())\n",
    "\n",
    "    projects = project_section.find_all('p', {'class': 'pv-accomplishment-entity__date t-14'})\n",
    "    project_time = []\n",
    "    for i in range(len(project_titles)):\n",
    "        try:\n",
    "            project_date = projects[i].get_text().split('\\n')[1]\n",
    "            project_date = re.sub(' +', ' ', project_date)\n",
    "            project_time.append(project_date[1:])\n",
    "        except Exception as e:\n",
    "            # print(\"project_date Exception\", e)\n",
    "            project_time.append('')\n",
    "\n",
    "    project_descriptions = []\n",
    "    projects2 = project_section.find_all('p', {'class': 'pv-accomplishment-entity__description t-14'})\n",
    "    for i in range(len(project_titles)):\n",
    "        try:\n",
    "            next_empty_elem = projects2[i].findNext('div')\n",
    "            curr_proj_desc = next_empty_elem.next_sibling\n",
    "            project_descriptions.append(curr_proj_desc.strip())\n",
    "        except Exception as e:\n",
    "            # print(\"curr_proj_desc Exception\", e)\n",
    "            project_descriptions.append('')\n",
    "\n",
    "    # Construct projects_info_list from above data\n",
    "    for i in range(len(project_titles)):\n",
    "        projects_info_list.append([project_titles[i], project_time[i], project_descriptions[i]])\n",
    "except Exception as e:\n",
    "    # no projects added\n",
    "    # print(\"Project Section Exception\", e)\n",
    "    pass\n",
    "# print(projects_info_list)\n",
    "\n",
    "# certifications section\n",
    "certifications_info_list = []\n",
    "try:\n",
    "    certificates_section = soup.find('section', {'id': 'certifications-section'})\n",
    "\n",
    "    list_items = certificates_section.find('ul',\n",
    "                                           {'class': 'pv-profile-section__section-info section-info pv-profile-section__section-info--has-more'})\n",
    "except Exception as e:\n",
    "    # print(\"certificates_section Exception\", e)\n",
    "    pass\n",
    "try:\n",
    "    if list_items is None:\n",
    "        list_items = certificates_section.find('ul', {\n",
    "            'class': 'pv-profile-section__section-info section-info pv-profile-section__section-info--has-no-more'})\n",
    "\n",
    "    items = list_items.find_all('li', {'class': 'pv-profile-section__sortable-item pv-certification-entity ember-view'})\n",
    "    cert_names_list = []\n",
    "    cert_issuer_list = []\n",
    "    cert_dates_list = []\n",
    "\n",
    "    for i in range(len(items)):\n",
    "        curr_cert_name = items[i].find('h3', {'class': 't-16 t-bold'})\n",
    "        curr_cert_name = curr_cert_name.get_text().strip()\n",
    "        cert_names_list.append(curr_cert_name)\n",
    "\n",
    "        curr_issuer_name = items[i].find_all('p', {'class': 't-14'})[0]\n",
    "        curr_issuer_name = curr_issuer_name.get_text().strip()\n",
    "        curr_issuer_name = curr_issuer_name.replace('Issuing authority\\n', '')\n",
    "        cert_issuer_list.append(curr_issuer_name)\n",
    "\n",
    "        curr_cert_date = items[i].find_all('p', {'class': 't-14'})[1]\n",
    "        curr_cert_date = curr_cert_date.get_text().strip()\n",
    "        curr_cert_date = curr_cert_date.replace('Issued date and, if applicable, expiration date of the certification or license\\n', '').replace(\n",
    "            'No Expiration Date', '').replace('Issued ', '')\n",
    "        cert_dates_list.append(curr_cert_date)\n",
    "\n",
    "    # adding elements in certifications_info_list as per schema\n",
    "    for i in range(len(cert_names_list)):\n",
    "        certifications_info_list.append([cert_names_list[i], cert_dates_list[i], cert_issuer_list[i]])\n",
    "\n",
    "except Exception as e:\n",
    "    # no certificates added\n",
    "    # print(\"Certificates Section Exception\", e)\n",
    "    pass\n",
    "\n",
    "# print(certifications_info_list)\n",
    "\n",
    "# Experience Section\n",
    "experience_info_list = []\n",
    "list_items = []\n",
    "items = []\n",
    "\n",
    "try:\n",
    "    experience_section = soup.find('section', {'class': 'experience-section'})\n",
    "    # print(experience_section)\n",
    "\n",
    "    list_items = experience_section.find('ul', {'class': 'pv-profile-section__section-info section-info pv-profile-section__section-info--has-more'})\n",
    "except Exception as e:\n",
    "    # print(\"experience_section Exception\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if list_items is None:\n",
    "        list_items = experience_section.find('ul',\n",
    "                                             {'class': 'pv-profile-section__section-info section-info pv-profile-section__section-info--has-no-more'})\n",
    "\n",
    "    items = list_items.find_all('li', {'class': 'pv-entity__position-group-pager pv-profile-section__list-item ember-view'})\n",
    "    company_names_list = []\n",
    "    position_list = []\n",
    "    dates_employed_list = []\n",
    "    description_list = []\n",
    "\n",
    "    for i in range(len(items)):\n",
    "        try:\n",
    "            curr_name = items[i].find('p', {'class': 'pv-entity__secondary-title t-14 t-black t-normal'})\n",
    "            curr_name = curr_name.get_text().strip()\n",
    "            curr_name = curr_name.split('\\n')[0].strip()\n",
    "            # print(\"1st currname\", curr_name)\n",
    "            company_names_list.append(curr_name)\n",
    "        except Exception as e:\n",
    "            # print(\"Experience curr_name Exception:\", e)\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if curr_name is None:\n",
    "                curr_name = items[i].find('h3', {'class': 't-16 t-black t-bold'})\n",
    "                curr_name = curr_name.get_text().strip()\n",
    "                curr_name = curr_name.replace(\"Company Name\\n\", '')\n",
    "                company_names_list.append(curr_name)\n",
    "        except Exception as e:\n",
    "            # print(\"Experience curr_name Exception:\", e)\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            curr_position = items[i].find('h3', {'class': 't-16 t-black t-bold'})\n",
    "            curr_position = curr_position.get_text().strip()\n",
    "            curr_name = curr_name.replace(\"Company Name\\n\", '')\n",
    "            position_list.append(curr_position)\n",
    "        except Exception as e:\n",
    "            # print(\"Experience curr_position Exception:\", e)\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            curr_dates = items[i].find('h4', {'class': 'pv-entity__date-range t-14 t-black--light t-normal'})\n",
    "            curr_dates = curr_dates.get_text().strip()\n",
    "            curr_dates = curr_dates.replace('Dates Employed\\n', '')\n",
    "            dates_employed_list.append(curr_dates)\n",
    "        except Exception as e:\n",
    "            # print(\"Experience curr_dates Exception:\", e)\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            curr_description = items[i].find('div', {'class': 'pv-entity__extra-details t-14 t-black--light ember-view'})\n",
    "            curr_description = curr_description.get_text().strip()\n",
    "            curr_description = curr_description.replace('\\n\\n\\n\\n\\n        see less', '')\n",
    "            curr_description = curr_description.replace('\\n\\n   \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', ' ')\n",
    "            curr_description = curr_description.replace('\\n\\n    \\n…\\n\\n        see more', '')\n",
    "            curr_description = curr_description.replace('\\n       ', '.')\n",
    "            curr_description = curr_description.replace('\\n\\n', '.')\n",
    "            description_list.append(curr_description)\n",
    "        except Exception as e:\n",
    "            # print(\"Experience curr_description Exception:\", e)\n",
    "            pass\n",
    "            # Add empty description for normalization of data\n",
    "            description_list.append('')\n",
    "\n",
    "    # create company_names_list from above data\n",
    "    for i in range(len(company_names_list)):\n",
    "        experience_info_list.append([company_names_list[i], position_list[i], dates_employed_list[i], description_list[i]])\n",
    "\n",
    "except Exception as e:\n",
    "    # No Experience Added\n",
    "    # print(\"Experience Section Exception:\", e)\n",
    "    pass\n",
    "# print(experience_info_list)\n",
    "\n",
    "\n",
    "# Skills Section\n",
    "skills_info_list = []\n",
    "try:\n",
    "    skills_section = soup.find('section', {'class': 'pv-profile-section pv-skill-categories-section artdeco-container-card ember-view'})\n",
    "except Exception as e:\n",
    "    # print(\"skills_section Exception\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if skills_section is None:\n",
    "        skills_section = soup.find('section',\n",
    "                                   {'class': 'pv-profile-section pv-skill-categories-section artdeco-container-card first-degree ember-view'})\n",
    "\n",
    "    all_skills = skills_section.find_all('span', {'class': 'pv-skill-category-entity__name-text t-16 t-black t-bold'})\n",
    "\n",
    "    for i in range(len(all_skills)):\n",
    "        skills_info_list.append(all_skills[i].get_text().strip())\n",
    "\n",
    "except Exception as e:\n",
    "    # No skills added\n",
    "    # print(\"Skills Section Exception:\", e)\n",
    "    pass\n",
    "\n",
    "# print(skills_info_list)\n",
    "\n",
    "\n",
    "# Volunteering Section:\n",
    "volunteer_info_list = []\n",
    "items = []\n",
    "list_items = []\n",
    "try:\n",
    "    volunteer_section = soup.find('section', {'class': 'pv-profile-section volunteering-section ember-view'})\n",
    "    list_items = volunteer_section.find('ul', {\n",
    "        'class': 'pv-profile-section__section-info section-info pv-profile-section__section-info--has-more ember-view'})\n",
    "except Exception as e:\n",
    "    # print(\"Volunteering volunteer_section Exception:\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if list_items is None:\n",
    "        list_items = volunteer_section.find('ul',\n",
    "                                            {'class': 'pv-profile-section__section-info section-info pv-profile-section__section-info--has-no-more'})\n",
    "except Exception as e:\n",
    "    # print(\"Volunteering list_items Exception:\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    items = list_items.find_all('li', {\n",
    "        'class': 'pv-profile-section__sortable-item pv-profile-section__section-info-item relative pv-profile-section__sortable-item--v2 pv-profile-section__list-item sortable-item ember-view'})\n",
    "except Exception as e:\n",
    "    # print(\"Volunteering list_items Exception:\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if items == []:\n",
    "        items = list_items.find_all('li', {'class': 'pv-profile-section__list-item pv-volunteering-entity pv-profile-section__card-item ember-view'})\n",
    "except Exception as e:\n",
    "    # print(\"Volunteering items Exception:\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    for i in range(len(items)):\n",
    "        curr_name = items[i].find('span', {'class': 'pv-entity__secondary-title'})\n",
    "        curr_name = curr_name.get_text().strip()\n",
    "\n",
    "        curr_role = items[i].find('h3', {'class': 't-16 t-black t-bold'})\n",
    "        curr_role = curr_role.get_text().strip()\n",
    "\n",
    "        try:\n",
    "            curr_dates = items[i].find('h4', {'class': 'pv-entity__date-range detail-facet inline-block t-14 t-black--light t-normal'})\n",
    "            curr_dates = curr_dates.get_text().strip()\n",
    "            curr_dates = curr_dates.replace('Dates volunteered\\n', '')\n",
    "        except Exception as e:\n",
    "            # print(\"curr_dates Exception\", e)\n",
    "            curr_dates = ''\n",
    "\n",
    "        try:\n",
    "            curr_description = items[i].find('p', {'class': 'pv-entity__description t-14 t-normal mt4'})\n",
    "            curr_description = curr_description.get_text().strip()\n",
    "        except Exception as e:\n",
    "            # print(\"curr_description Exception\", e)\n",
    "            curr_description = ''\n",
    "\n",
    "        # Construct volunteer_info_list from above data\n",
    "        volunteer_info_list.append([curr_name, curr_role, curr_dates, curr_description])\n",
    "\n",
    "except Exception as e:\n",
    "    # no volunteering added\n",
    "    # print(\"Volunteering Section Exception\", e)\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # click to expand honors and awards section because only either projects or honors and awards can be expanded at a time\n",
    "    honors_and_awards_expand_button = browser.find_element_by_xpath(\n",
    "        \"//section[@class='pv-profile-section pv-accomplishments-section artdeco-container-card ember-view']//button[@aria-label='Expand honors & awards section']\")\n",
    "    browser.execute_script(\"arguments[0].click();\", honors_and_awards_expand_button)\n",
    "\n",
    "    # click to expand honors and awards section to show more\n",
    "    honors_and_awards_expand_button2 = browser.find_element_by_xpath(\n",
    "        \"//section[@class='pv-profile-section pv-accomplishments-section artdeco-container-card ember-view']//button[@aria-controls='honors-expandable-content' and @aria-expanded='false']\")\n",
    "    browser.execute_script(\"arguments[0].click();\", honors_and_awards_expand_button2)\n",
    "except Exception as e:\n",
    "    # print(\"honors_and_awards_expand_button Exception\", e)\n",
    "    pass\n",
    "\n",
    "# accomplishments section\n",
    "accomplishments_info_list = []\n",
    "try:\n",
    "    accomplishments_section = soup.find_all('section', {'class': 'pv-profile-section pv-accomplishments-section artdeco-container-card ember-view'})\n",
    "\n",
    "    honors_section = accomplishments_section[0].find('div', {'aria-labelledby': 'honors-title'})\n",
    "\n",
    "    list_items = honors_section.find_all('li', {'class': 'pv-accomplishments-block__summary-list-item'})\n",
    "\n",
    "    for i in range(len(list_items)):\n",
    "        accomplishments_info_list.append(list_items[i].get_text().strip())\n",
    "\n",
    "except Exception as e:\n",
    "    # No accomplishments added\n",
    "    # print(\"Accomplishments Section Exception\", e)\n",
    "    pass\n",
    "\n",
    "# Close the browser once scraping is done\n",
    "browser.close()\n",
    "\n",
    "# TESTING OUTPUTS\n",
    "# print(\"LISTS\")\n",
    "# print(basic_info_list)\n",
    "# print(education_info_list)\n",
    "# print(projects_info_list)\n",
    "# print(certifications_info_list)\n",
    "# print(experience_info_list)\n",
    "# print(skills_info_list)\n",
    "# print(volunteer_info_list)\n",
    "# print(accomplishments_info_list)\n",
    "\n",
    "final_all_lists = [basic_info_list, education_info_list, projects_info_list, certifications_info_list, experience_info_list, skills_info_list,\n",
    "                   volunteer_info_list, accomplishments_info_list]\n",
    "\n",
    "json_data = {'basic_info_list': basic_info_list, 'education_info_list': education_info_list, 'projects_info_list': projects_info_list,\n",
    "             'certifications_info_list': certifications_info_list, 'experience_info_list': experience_info_list, 'skills_info_list': skills_info_list,\n",
    "             'volunteer_info_list': volunteer_info_list, 'accomplishments_info_list': accomplishments_info_list, }\n",
    "\n",
    "final_json_string = json.dumps(json_data, indent=4)\n",
    "print(final_json_string)\n",
    "\n",
    "fileheader = open(\"scraped_data.json\", 'w')\n",
    "\n",
    "fileheader.writelines(final_json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f9986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "url='https://simple.wikipedia.org/wiki/List_of_U.S._states'\n",
    "web=urlopen(url)\n",
    "source=BeautifulSoup(web, 'html.parser')\n",
    "table=source.find('table', {'class': 'wikitable sortable jquery-tablesorter'})\n",
    "abbs=table.find_all('b')\n",
    "print(abbs.get_text())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "table = source.find('table', class_='wikitable')\n",
    "abbs = table.find_all('b')\n",
    "\n",
    "abbs_list = [i.get_text().strip() for i in abbs]\n",
    "print(abbs_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
